{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron with Dense Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_EPOCH = 50\n",
    "DEFAULT_BATCH = 72\n",
    "\n",
    "VALIDATION_SPLIT = 0.3\n",
    "TESTING_SPLIT = 0.3\n",
    "\n",
    "TOTAL = 744\n",
    "N_TRAIN = 550"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set a default batch size value and number of epochs. Also, the training and testing datasets have a fixed size. The former is defined by the N_TRAIN variable, whilst the latter is TOTAL-N_TRAIN\n",
    "\n",
    "VALIDATION_SPLIT and TESTING_SPLIT denote respectively the percentage of training that is used during the validation step and the percentage of inputs that is used during the evaluation step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Dense MLP Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data processor and dataset are respectively defined as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.processor = processor #The data processor is responsible for handling operations with the dataset such as scaling, rescaling and preparing the data from a time series table to a supervised learning approach\n",
    "self.values = values #The values variable is the result of the values from the csv file read by the Pandas package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data processor is responsible for handling operations with the dataset such as scaling, rescaling and preparing the data from a time series table to a supervised learning approach\n",
    "\n",
    "The values variable is the result of the values from the csv file read by the Pandas package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.hours = args.hours if args.hours else 1\n",
    "self.checkpoint = args.checkpoint if args.checkpoint else \"dense_\" + len(args.neurons) + \"_layers_checkpoint.keras\"\n",
    "self.epochs = args.epochs if args.epochs else DEFAULT_EPOCH\n",
    "self.batch  = args.batch if args.batch else DEFAULT_BATCH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to set some variable using the arguments when calling the main.py from the command-line, such as the number of hours to consider for prediction (e.g. 1, 2 ou 3 hours into the future), the number of epochs, the batch size and others.\n",
    "\n",
    "The checkpoint variable represents the filename that contains the weights obtained during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training and Testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.build_data()\n",
    "\n",
    "def build_data(self):\n",
    "    inputs = self.values[0:N_INPUTS]\n",
    "    prediction = self.values[N_INPUTS:TOTAL]\n",
    "    \n",
    "    np.random.shuffle(inputs)\n",
    "    N_TEST = int(N_INPUTS*TESTING_SPLIT)\n",
    "\n",
    "    self.train_X, self.train_y = inputs[:(N_INPUTS-N_TEST), :-1], inputs[:(N_INPUTS-N_TEST), -1]\n",
    "    self.test_X, self.test_y = inputs[(N_INPUTS-N_TEST):, :-1], inputs[(N_INPUTS-N_TEST):, -1]\n",
    "\n",
    "    self.prediction_X, self.prediction_y = prediction[:, :-1], prediction[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call the build_data() method in the class constructor. This method separates the self.values variable into training and testing arrays according to the number of training and testing samples specified previously.\n",
    "\n",
    "Considering the prediction of a single attribute within the next hour, the training and testing arrays have n+1 elements, in which n is the number of inputs. Considering now an ANN with 9 inputs and 1 output, the inputs for the ANN are represented from the 1st to the 9th element, whilst the 10th element is the next hour value to be predicted. Therefore, we set a self.train_x as the inputs (from the 1st to the 9th element) to be used during training and self.train_y as the labels (the 10th element) to be used during training. We apply the same logic for the testing array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model and Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.model = keras.Sequential()\n",
    "\n",
    "self.model.add(keras.layers.Dense(\n",
    "      units=args.neurons[0],\n",
    "      activation=\"tanh\",\n",
    "      input_shape=(args.input,)\n",
    "))\n",
    "\n",
    "for key, neurons in enumerate(args.neurons):\n",
    "    if key < len(args.neurons) - 1:\n",
    "        self.model.add(keras.layers.Dense(units=args.neurons[key+1], activation=\"tanh\"))\n",
    "    else:\n",
    "        self.model.add(keras.layers.Dense(units=args.output, activation=\"tanh\"))\n",
    "\n",
    "self.model.add(keras.layers.Dense(units=args.output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define our Keras model as a Sequential from the keras package. Then we define the ann's layers. It is mandatory to have, at least, three layers, in which: 1 is the input layer, 1 is the hidden layer and 1 is the output layer.\n",
    "\n",
    "The activation function for all layers, except the output layer, is the hyperbolic tangent. The output layer uses a linear activation function.\n",
    "\n",
    "The number of hidden layers and, consequently, the number of neurons for each hidden layer are determined by the flag -n (--neurons). For example, if we define *-n 6 5* the ANN ought to have 2 hidden layers containing respectively 6 and 5 neurons each.\n",
    "\n",
    "The layers are defined as the Keras' Dense layer. The shape of the first layer input is defined as a (n_inputs,) tuple. Also this layer produces as many outputs as there are neurons for the next hidden layer, i.e., in the previous example (-n 6 5) each node of the input layer produces 6 outputs (each one connected to the next layer's neuron). The same logic applies for the each hidden layer, except the last one. That is: each node of the first hidden layer ought to produce 5 outputs. If our example had more hidden layers the 2nd hidden layer should produce as output the number of neurons belonging to a third hidden layer. However, since the 2nd hidden layer is the last one before the output layer, it produces a number of outputs according to the number of neurons for the output layer. The last layer is the output layer and it produces a number of n_outputs output(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(self):\n",
    "    print(\"train\")\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "      filepath=self.checkpoint,\n",
    "      monitor=\"loss\",\n",
    "      verbose=1,\n",
    "      save_weights_only=True,\n",
    "      save_best_only=True\n",
    "    )\n",
    "    early_stopping = keras.callbacks.EarlyStopping(monitor=\"loss\", patience=5, verbose=1)\n",
    "    tensorboard = keras.callbacks.TensorBoard(log_dir=\"./logs/\", histogram_freq=0, write_graph=False)\n",
    "    \n",
    "    callbacks = [\n",
    "      checkpoint, early_stopping, tensorboard\n",
    "    ]\n",
    "    \n",
    "    history = self.model.fit(\n",
    "      self.train_X, self.train_y, epochs=self.epochs,\n",
    "      steps_per_epoch=72,\n",
    "      validation_steps=10,\n",
    "      validation_split=VALIDATION_SPLIT,\n",
    "      callbacks=callbacks\n",
    "    )\n",
    "\n",
    "    pyplot.plot(history.history['loss'], label='train')\n",
    "    pyplot.plot(history.history['val_loss'], label='validation')\n",
    "    pyplot.legend()\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define three callbacks functions before training the model.\n",
    "\n",
    "The checkpoint callback is responsible for dinamically saving the weights during the training step as soon as a improvement in the loss value (the variable being monitored) is detected. The filepath is defined by the self.checkpoint variable presented previously.\n",
    "\n",
    "The early stopping checkpoint also monitors the loss value during training and it is meant to stop the training step as soon as no improvement is detected in the monitored value.\n",
    "\n",
    "The tensorboard checkpoint generates logs of events to be visualised using Tensorboard.\n",
    "\n",
    "Finally, the model is trained using the fit() method. We supply self.train_X and self.train_y as respectively the training data and the training label data. The number of epochs are determined by self.epochs and can either be the default value (50 epochs) or user-defined through the -e flag. Each epoch takes 36 steps to finish. Currently the whole training data and training label data are being used for validation (see validation_split=1).\n",
    "\n",
    "It is important to say that the label array contains only values for prediction within the next hour, that is: assuming that the 1th element of the training array references contents of the 13th hour of any given date, the 1th element of the label array will, therefore, contain an attribute referencing the 14th hour of the same day.\n",
    "\n",
    "At the end of the training the graphic containing both the loss and validation loss values is plotted. We can use this graphic to investigate whether the training overfitted or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(self):\n",
    "    result = self.model.evaluate(x=self.train_X, y=self.train_y)\n",
    "\n",
    "    for res, metric in zip(result, self.model.metrics_names):\n",
    "      print(\"{0}: {1:.3e}\".format(metric, res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model evaluation occurs using both the training data and the training label data. The evaluation for each metric used is then printed in scientific notation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self):\n",
    "    print(\"predict\")\n",
    "\n",
    "    y_reshaped, y_real = None, self.processor.rescale(self.prediction_X[:,4].reshape(len(self.prediction_X), 1), self.prediction_X)\n",
    "\n",
    "    for hour in range(self.hours):\n",
    "      self.prediction_X = self.prediction_X[1:] if hour > 1 else self.prediction_X[0:]\n",
    "      y_output = self.model.predict(self.prediction_X)\n",
    "\n",
    "      self.prediction_X[:,4] = y_output.reshape(len(y_output))\n",
    "\n",
    "      y_reshaped = self.processor.rescale(y_output, self.prediction_X)\n",
    "    \n",
    "    index_ = self.hours-2 if self.hours > 1 else 0\n",
    "    rmse = sqrt(mean_squared_error(y_reshaped, y_real[index_:]))\n",
    "    print('RMSE: %.3f' % rmse)\n",
    "\n",
    "    y = np.zeros(y_real.shape)\n",
    "    np.put(y, np.indices(y.shape), np.nan)\n",
    "    starting_index = len(y) - len(y_reshaped)\n",
    "\n",
    "    np.put(y, np.indices(y.shape)[:,starting_index:],y_reshaped)\n",
    "\n",
    "    pyplot.plot(y, label='predicted')\n",
    "    pyplot.plot(y_real, label='measured')\n",
    "    pyplot.legend()\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A prediction can occur for the next 1, 2 or n hours. The number of hours is specified using the flag -hours (--hours) when executing the main script and then it is saved using the self.hours variable.\n",
    "\n",
    "Since the model is training considering only the next hours, if a +12 hours prediction is needed, it is necessary to first predict the previous 11 hours (one by one).\n",
    "\n",
    "Finally, after predicting the last hour, the results are rescaled and then plotted in a graphic.\n",
    "\n",
    "There were some adjustment to the graphic in order to display all 194 testing labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "DEFAULT_EPOCH = 50\n",
    "DEFAULT_BATCH = 72\n",
    "\n",
    "VALIDATION_SPLIT = 0.3\n",
    "TESTING_SPLIT = 0.3\n",
    "\n",
    "TOTAL = 744\n",
    "N_INPUTS = 550\n",
    "\n",
    "class KerasDenseMLP:\n",
    "\n",
    "  def __init__(self, processor, args = None, values = []):\n",
    "\n",
    "    self.processor = processor\n",
    "    self.values = values\n",
    "\n",
    "    self.hours = args.hours if args.hours else 1\n",
    "    self.checkpoint = args.checkpoint if args.checkpoint else \"dense_\" + str(len(args.neurons)) + \"_layers_checkpoint.keras\"\n",
    "    self.epochs = args.epochs if args.epochs else DEFAULT_EPOCH\n",
    "    self.batch  = args.batch if args.batch else DEFAULT_BATCH\n",
    "\n",
    "    self.features = args.input\n",
    "\n",
    "    self.build_data()\n",
    "    self.model = keras.Sequential()\n",
    "    \n",
    "    self.model.add(keras.layers.Dense(\n",
    "      units=args.neurons[0],\n",
    "      activation=\"tanh\",\n",
    "      input_shape=(args.input,)\n",
    "    ))\n",
    "\n",
    "    for key, neurons in enumerate(args.neurons):\n",
    "      if key < len(args.neurons) - 1:\n",
    "        self.model.add(keras.layers.Dense(units=args.neurons[key+1], activation=\"tanh\"))\n",
    "      else:\n",
    "        self.model.add(keras.layers.Dense(units=args.output, activation=\"tanh\"))\n",
    "\n",
    "    self.model.add(keras.layers.Dense(units=args.output))\n",
    "    \n",
    "    print(self.model.summary())\n",
    "    self.model.compile(\n",
    "      optimizer=keras.optimizers.SGD(lr=args.learning),\n",
    "      loss=keras.losses.MSE,\n",
    "      metrics=[keras.metrics.MSE, keras.metrics.MAE]\n",
    "    )\n",
    "\n",
    "  def build_data(self):\n",
    "    inputs = self.values[0:N_INPUTS]\n",
    "    prediction = self.values[N_INPUTS:TOTAL]\n",
    "    \n",
    "    np.random.shuffle(inputs)\n",
    "    N_TEST = int(N_INPUTS*TESTING_SPLIT)\n",
    "\n",
    "    self.train_X, self.train_y = inputs[:(N_INPUTS-N_TEST), :-1], inputs[:(N_INPUTS-N_TEST), -1]\n",
    "    self.test_X, self.test_y = inputs[(N_INPUTS-N_TEST):, :-1], inputs[(N_INPUTS-N_TEST):, -1]\n",
    "\n",
    "    self.prediction_X, self.prediction_y = prediction[:, :-1], prediction[:, -1]\n",
    "\n",
    "\n",
    "\n",
    "  def train(self):\n",
    "    print(\"train\")\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "      filepath=self.checkpoint,\n",
    "      monitor=\"val_loss\",\n",
    "      verbose=1,\n",
    "      save_weights_only=True,\n",
    "      save_best_only=True\n",
    "    )\n",
    "    early_stopping = keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, verbose=1)\n",
    "    tensorboard = keras.callbacks.TensorBoard(log_dir=\"./logs/\", histogram_freq=0, write_graph=False)\n",
    "    reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.1,\n",
    "        min_lr=1e-4,\n",
    "        patience=0,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    callbacks = [\n",
    "      checkpoint, early_stopping, tensorboard, reduce_lr\n",
    "    ]\n",
    "    \n",
    "    history = self.model.fit(\n",
    "      self.train_X, self.train_y, epochs=self.epochs,\n",
    "      steps_per_epoch=200,\n",
    "      validation_steps=10,\n",
    "      validation_split=VALIDATION_SPLIT,\n",
    "      callbacks=callbacks\n",
    "    )\n",
    "\n",
    "    pyplot.plot(history.history['loss'], label='train')\n",
    "    pyplot.plot(history.history['val_loss'], label='validation')\n",
    "    pyplot.legend()\n",
    "    pyplot.show()\n",
    "\n",
    "  def evaluate(self):\n",
    "    print(\"eval\")\n",
    "    result = self.model.evaluate(x=self.test_X, y=self.test_y)\n",
    "\n",
    "    for res, metric in zip(result, self.model.metrics_names):\n",
    "      print(\"{0}: {1:.3e}\".format(metric, res))\n",
    "\n",
    "  def predict(self):\n",
    "    print(\"predict\")\n",
    "\n",
    "    y_reshaped, y_real = None, self.processor.rescale(self.prediction_X[:,4].reshape(len(self.prediction_X), 1), self.prediction_X)\n",
    "\n",
    "    for hour in range(self.hours):\n",
    "      self.prediction_X = self.prediction_X[1:] if hour > 1 else self.prediction_X[0:]\n",
    "      y_output = self.model.predict(self.prediction_X)\n",
    "\n",
    "      self.prediction_X[:,4] = y_output.reshape(len(y_output))\n",
    "\n",
    "      y_reshaped = self.processor.rescale(y_output, self.prediction_X)\n",
    "    \n",
    "    index_ = self.hours-2 if self.hours > 1 else 0\n",
    "    rmse = sqrt(mean_squared_error(y_reshaped, y_real[index_:]))\n",
    "    print('RMSE: %.3f' % rmse)\n",
    "\n",
    "    y = np.zeros(y_real.shape)\n",
    "    np.put(y, np.indices(y.shape), np.nan)\n",
    "    starting_index = len(y) - len(y_reshaped)\n",
    "\n",
    "    np.put(y, np.indices(y.shape)[:,starting_index:],y_reshaped)\n",
    "\n",
    "    pyplot.plot(y, label='predicted')\n",
    "    pyplot.plot(y_real, label='measured')\n",
    "    pyplot.legend()\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
